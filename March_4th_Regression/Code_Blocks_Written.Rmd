---
title: "Linear Modeling and Regression: Capabilities in R"
output:
  html_document:
    df_print: paged
  pdf_document:
    keep_tex: yes
    
---


<!--- Bookdown attempt_1 --->
```{r include=FALSE}
attach(mtcars)
library(ggplot2)

```


REMEMBER: Comment why; not what  
And name functions & variables for their purpose

___
These images are brought to you by the stress reducing statistics site: [__statswithcats.wordpress.com__](https://statswithcats.wordpress.com/)

<!-- ![](https://statswithcats.files.wordpress.com/2019/01/2-1-rskgilop2zb21.jpg) -->



### Contents:
I hope to cover for Regression Analysis. Followed by a demonstration in linear modeling. 
Our final task will be to perform multiple linear regression to see which genes best model the difference between two strains of mice `(C57BL/6J and DBA/2J)`. Along the way, I might cover polynomial regression and multi-variable regression; but we will have to be judicious with our time. We can cover them at the end or at your own time. 

 1. Regression and Linear Modeling
 2. Adjacent modeling types
 3. polynomial and multi-variable regression
 4. Multiple Linear regression in mice strain GWAS differentiation

___

<!--- Stats quick reference --->
## Previously:

<div class = "row">
<div class = "col-md-6">
We covered **Correlation Coefficients (r)**.  
 - also called the Pearson product moment correlation coefficient

Important properties:  
<!-- <br /> -->
range from -0 to 1  
<!-- <br /> -->
Positive value means that y increases with x  
<!-- <br /> -->
Negative value means that y decreases with x  
<!-- <br /> -->

This helps us to look at our data and know if there is a trend buried within. Whatever it may be, our eyes are very good at noticing trends, but if we want a machine to pick it out then we have to look at R^2^.   

We know that all datasets are not created equal. Many statistical attributes are effected by sample size, measurement variance, and outliers. These are things to be wary of especially in the assumptions we have prior to performing regression.  
Some key assumptions include:  

1. The relationship between the dependent and independent variable is roughly linear  
2. The variable are multivariate normal
3. There is little multicollinearity between the independent variables.  

</div>
<div class = "col-md-6">
![](https://statswithcats.files.wordpress.com/2019/01/linear-curvilinear-trends.png)

</div>
</div>

![](https://statswithcats.files.wordpress.com/2019/01/outliers-b.png)


___

## Introduction to Data Modeling:

In a regression model we try to ascertain the variation in our dependent Y variable that is being caused by some independent X variable. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). ( [r4ds](https://r4ds.had.co.nz/model-intro.html) )  
The uses for regression are numerous with the most common being predictive followed by hypothesis confirmation and exploratory. The goal of which is always; how well can we generalize from this specific information.  

___

#### This section will cover an example with [mtcars](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/mtcars) a built-in dataset contained in R. 

The goal here is to gain some information on fuel efficiency from the information in the dataset. Let's explore the dataset and see which variables may be useful in predicting the MPG column.

```{r}
mtcars
```


  
  


```{r}
pairs(mtcars[,c(1,3:4)])

```


___

## Before we get too far:
How do we build and interpret models?  

We need to find the good models by making precise our intuition that a good model is “close” to the data. This distance is just the difference between the y value given by the model (the prediction), and the actual y value in the data (the response).
$$MSE = \frac{1}{n}\sum_{i=1}^n (Y_i - \bar{Y})^2$$

___

<div class = "row">
<div class = "col-md-6">

![](https://d33wubrfki0l68.cloudfront.net/97986d02df194d0dc367d9651c0eed1dac156572/93749/model-basics_files/figure-html/unnamed-chunk-8-1.png)

</div>
<div class = "col-md-6">

![](https://d33wubrfki0l68.cloudfront.net/1dc0273cd5043e45f229dc49c870c568648529ff/8ebcb/model-basics_files/figure-html/unnamed-chunk-4-1.png)

</div>
</div>



```{r}
summary(lm(mpg ~ ., mtcars))
summary(lm(mpg ~ wt, mtcars))
```

___

**Call:** This is an R feature that shows what function and parameters were used to create the model.  
**Residuals:** Difference between what the model predicted and the actual value of y.  
**Coefficients:** These are the weights that minimize the sum of the square of the errors.  
 <br /> 

**Residual Standard Error:** Essentially standard deviation of residuals / errors of your regression model.  
**Multiple R-Squared:** Percent of the variance of Y intact after subtracting the error of the model.  
**Adjusted R-Squared:** Same as multiple R-Squared but takes into account the number of samples and variables you’re using.  
**F-Statistic:** Global test to check if your model has at least one significant variable.  Takes into account number of variables and observations used.  



[Detailed Explanation of Summary()](http://www.learnbymarketing.com/tutorials/linear-regression-in-r/)

___

```{r}
# We could possible compare the two or plot the resituals or other values from the fitted model
# fit1 <- lm(mpg ~ . , mtcars)
# fit2 <- lm(mpg ~ wt, mtcars)
#   
# anova(fit1, fit2)   # Compare models
# 
# plot(fit2)          # Plot the parameters of the fitted model

plot(mpg ~ wt, data = mtcars) + 
  abline( lm(mpg~wt, data = mtcars))

```


___

What about the other variabels. Do they correctly delineate the fuel efficiency?

___

```{r}
p_values <- rep(0,dim(mtcars)[2]-1)
for( i in 2:dim(mtcars)[2] ){  # 11 columns - 1 mpg column
  variable <- mtcars[,i]  # extract column
  fit <- lm( mpg ~ variable, data = mtcars)  # linear-fit
  p_values[i] <- summary(fit)$coefficients[2,4]  # now all p-values of the GLM fit can be found
}

# This correction is not strictly neccessary but would help in instances of multiple comparisons (same item tests)
adj_p_values = p.adjust(p_values, "bonferroni")
names(adj_p_values) <- names(mtcars)[2:11]

# -log10(p-values): A Manhattan Plot
plot(-log10(adj_p_values), main = "Linear Regression analysis of MPG", xlab = "Column name", ylab = "-log10(p-values)", sub = "lm for significant relationship to x-axis variables", xaxt = "n")
axis(1,at=2:12,labels=names(adj_p_values))
```


<center>
ENDING THE FIRST
</center>






___

### Multiple Variable Regression:
If we were to only include one variable in our model it would likely be `WT`. But if we could include more, and perhaps build a more accurate model, which would we include. A step function moves us in the right direction by incrementally adding variables that make the most impact to our model.  
Note: AIC is value that is helpful for comparing models as it includes measures of both how well the model fits the data and how complex the model is.

___
```{r}
min.model <- ( lm(mpg~1, data = mtcars) )
biggest <- formula( lm(mpg~., mtcars) )
# > biggest
# > mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb

fwd.model = step(min.model, direction='forward', scope=biggest)
```

___

We have identified the best three variables to model MPG after: `mpg ~ wt + cyl + hp`

___
```{r}
##### Multivariate linear fit and prediction
fit <- lm(mpg ~ wt + cyl + hp, data = mtcars)
summary(fit)

# This will smooth the line
p_wt <- seq(min(wt), max(wt), length.out=100)
p_cyl <- seq(min(cyl), max(cyl), length.out=100)
p_hp <- seq(min(hp), max(hp), length.out=100)
p_data = data.frame( wt = p_wt, cyl = p_cyl, hp = p_hp )
p <- predict(fit, newdata = p_data)

##### Plot
ggplot(mtcars, aes(x = wt + cyl + hp, y = mpg)) + 
  geom_point() + 
  geom_line(data = as.data.frame(p_data), aes(p_wt + p_cyl + p_hp, p)) + 
  # geom_smooth(method = "loess") +
  theme_light() +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     # " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)))
```
___

### Polynomial Example 

___


```{r}

ggplot(mtcars, aes(x = hp, y = mpg)) + 
  geom_point() + 
  # geom_line() + 
  geom_smooth(method = "lm") + 
  theme_light()


##### Linear fit and prediction
fit <- lm( mpg ~ hp )
summary(fit)

p_hp <- seq(min(hp), max(hp), length.out=100)
p_data = data.frame( hp = p_hp )
p <- predict(fit, newdata = data.frame( hp = p_hp ))

##### Plot
ggplot(mtcars, aes(x = hp, y = mpg)) + 
  geom_point() + 
  geom_line(data = as.data.frame(p_data), aes(p_hp, p)) + 
  # geom_smooth(method = "lm") + 
  theme_light() +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     # " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)))


##### polynomial fit and prediction
fit <- lm(mpg ~ hp + I(hp^2))
summary(fit)

p_hp <- seq(min(hp), max(hp), length.out=100)
p_data = data.frame( hp = p_hp )
p <- predict(fit, newdata = data.frame( hp = p_hp ))

##### Plot
ggplot(mtcars, aes(x = hp, y = mpg)) + 
  geom_point() + 
  geom_line(data = as.data.frame(p_data), aes(p_hp, p)) + 
  # geom_smooth(method = "loess") +
  theme_light() +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     # " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)))



```

___

### Multivariate polynomial fit and prediction
Combine the previous two explorations

___

```{r}
##### Multivariate polynomial fit and prediction
fit <- lm(mpg ~ wt + I(wt^2) + hp + I(hp^2))
summary(fit)

# This will smooth the line
p_wt <- seq(min(wt), max(wt), length.out=100)
p_wt2 <- seq(min(wt), max(wt^2), length.out=100)
# p_cyl <- seq(min(cyl), max(cyl), length.out=100)
p_hp <- seq(min(hp), max(hp), length.out=100)
p_hp2 <- seq(min(hp), max(hp^2), length.out=100)
p_data = data.frame( wt = p_wt, `I(wt^2)` = p_wt2, 
                                        hp = p_hp, `I(hp^2)` = p_hp2 )
p <- predict(fit, newdata = p_data)

##### Plot
ggplot(mtcars, aes(x = wt + hp, y = mpg)) + 
  geom_point() + 
  geom_line(data = as.data.frame(p_data), aes(p_wt + p_hp, p)) + 
  # geom_smooth(method = "loess") +
  theme_light() +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     # " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)))
```




___

## Ending the Second

___

Perform a regression-based analysis to identify genes that are diﬀerentially expressed between
the two mouse strains using Poisson regression. Plot the appropriate -log10 p-values from
your analysis.

As always the first order of business is to load the Rdata and the respective libraries I will be using for the problem.
```{r}
library("MASS")
library("foreign")
library("ggplot2")
 
# load data
load("assessment_data.Rdata")

```


```{r}
# lets make our variables more readable!::
 
strain_group <- X # (C57BL/6J and DBA/2J)
gene_count <- dim(Y)[1] # 8544 genes
mouse_count <- dim(Y)[2] # 21 mice
 
# The comparison from the generalized linear model (GLM) will need a row of count data from the matrix and the strain_group which will give the model a reference for comparison. So that gene_expression ~ strain_group
 
gene_expression <- Y[1,] # row data for a single gene containing counts for each mouse

 
# Construct a model for comparison base on strain type. 
glm.fit = glm.nb(gene_expression ~ strain_group, link = log)

 
# The value of particular interest is the summary coefficient of the p-value contained in [2,4]
p_value <- summary(glm.fit)$coefficients[2,4]
print(p_value)


```


## Ending the Third





The ﬁle contains gene expression count data for 8,544 genes for 21 mice. These counts are contained in the matrix Y. The mice are from two strains (C57BL/6J and DBA/2J) and this information is contained in the vector X.













